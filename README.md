# ðŸš€ Azure Data Engineering: Customer Churn Pipeline (Modern Data Warehouse)

## ðŸ“Œ DescripciÃ³n del Proyecto
Este proyecto simula un escenario real del sector de telecomunicaciones implementando un **Modern Data Warehouse** en Azure bajo una **Arquitectura MedallÃ³n**. El objetivo principal fue construir un pipeline de datos (ETL) automatizado que procese informaciÃ³n cruda de clientes, aplique reglas de limpieza y seguridad (anonimizaciÃ³n), y disponibilice los datos en un modelo dimensional para descubrir patrones de abandono (Churn) a travÃ©s de inteligencia de negocios.

## ðŸ› ï¸ Stack TecnolÃ³gico
* **Procesamiento y ETL:** Python (Pandas, Hashlib) ejecutado en Visual Studio Code.
* **Almacenamiento Big Data:** Azure Data Lake Storage Gen2 (ADLS Gen2).
* **OrquestaciÃ³n y Movimiento:** Azure Data Factory (ADF).
* **Almacenamiento Relacional:** Azure SQL Database.
* **Consumo y BI:** Power BI Desktop.

## â˜ï¸ Infraestructura en la Nube
Para este proyecto se provisionÃ³ un grupo de recursos (`rg-medallon`) que centraliza los servicios administrados (PaaS) de orquestaciÃ³n, almacenamiento estructurado y no estructurado:

> <img width="1907" height="869" alt="GrupoRecurso" src="https://github.com/user-attachments/assets/ebe05d7e-dc96-4bf6-8d1f-61b99591b6c3" />

---

## ðŸ—ï¸ Arquitectura del Pipeline (Medallion Architecture)

### 1. Capa Bronce (Landing Zone)
Se estableciÃ³ el contenedor `bronze` en **Azure Data Lake Storage Gen2** (`datalakeproject2026`) como zona de aterrizaje. AquÃ­ se ingiriÃ³ el dataset original (`.csv` con +7,000 registros), manteniÃ©ndolo en su estado crudo e inmutable para preservar el historial de origen.

> <img width="1916" height="728" alt="AlmacenamientoContenedores" src="https://github.com/user-attachments/assets/3c1784e6-2bfd-4385-8356-2ef8ab229119" />


### 2. Capa Plata (TransformaciÃ³n y Limpieza con Python)
Se desarrollÃ³ un script de **Python (Pandas)** para ejecutar las siguientes reglas de negocio y calidad de datos:
* **Limpieza:** IdentificaciÃ³n y eliminaciÃ³n de registros duplicados (`drop_duplicates`) y manejo de valores inconsistentes.
* **EstandarizaciÃ³n:** NormalizaciÃ³n de variables categÃ³ricas (ej. traducciÃ³n de mÃ©todos de pago y contratos al espaÃ±ol para facilitar el consumo del usuario final).
* **Gobernanza y Seguridad (PII):** AplicaciÃ³n de una funciÃ³n de encriptaciÃ³n (Hashing SHA-256) a la columna `CustomerID` para proteger la Identidad Personal del cliente, cumpliendo con las mejores prÃ¡cticas de privacidad.
* **OptimizaciÃ³n:** ExportaciÃ³n del dataframe resultante al contenedor `silver` en formato columnar **Parquet**, reduciendo drÃ¡sticamente el peso del archivo y acelerando los tiempos de lectura.

PARTE 1
> <img width="1901" height="1002" alt="ScriptPy-1" src="https://github.com/user-attachments/assets/f78b8316-e458-4d23-934a-d0c0c2744385" />
PARTE 2
> <img width="1860" height="856" alt="ScriptPy-2" src="https://github.com/user-attachments/assets/7ad01575-4050-40ce-a035-ac17b6ace0d4" />


### 3. Capa Oro (OrquestaciÃ³n con Azure Data Factory)
DiseÃ±o de canalizaciones (Pipelines) automatizadas en `adf-telco-project-2026` para el movimiento de datos hacia la capa de consumo:
* ConfiguraciÃ³n de *Linked Services* y *Datasets* dinÃ¡micos para la conexiÃ³n segura entre el Data Lake y Azure SQL.
* Uso de actividades **Copy Data** secuenciales (dependencias de Ã©xito) para respetar la integridad referencial del modelo dimensional.
* ImplementaciÃ³n de **Idempotencia** mediante scripts de truncaciÃ³n de destino (`TRUNCATE TABLE`) previos a la copia, asegurando que el pipeline soporte cargas completas (Full Load) repetitivas sin colisiones de llaves primarias.

> <img width="1882" height="833" alt="ADF-CANALIZACION" src="https://github.com/user-attachments/assets/968ea094-c9c0-49e4-b626-b6d33c6d3a4c" />


### 4. Almacenamiento Estructurado (Azure SQL Database)
ImplementaciÃ³n de un **Modelo en Estrella (Star Schema)** utilizando DDL (Data Definition Language) en `sqldb-telco-gold`:
* **Dim_Customer:** Tabla dimensional que almacena atributos demogrÃ¡ficos (gÃ©nero, edad, dependientes).
* **Fact_Churn:** Tabla de hechos que centraliza las mÃ©tricas financieras (cargos mensuales/totales), la vigencia del servicio y el indicador binario de abandono.

> <img width="1894" height="892" alt="ConsultaSQL" src="https://github.com/user-attachments/assets/ab4b0e33-34f7-42af-9a46-7ce6e1b08d5d" />

---

## ðŸ“Š Business Intelligence y Resultados (Power BI)
Se conectÃ³ un modelo semÃ¡ntico en Power BI mediante importaciÃ³n directa desde la base de datos SQL. Se modelÃ³ una relaciÃ³n `1:N` entre la dimensiÃ³n y la tabla de hechos para crear un Dashboard Ejecutivo que revelÃ³ los siguientes insights accionables:
* **MÃ©trica Principal:** Una Tasa de Abandono Total del **26.54%**.
* **Alerta de Early Churn:** IdentificaciÃ³n de un pico crÃ­tico de pÃ©rdida de clientes durante el primer mes de servicio, sugiriendo fallas en el proceso de *onboarding* o expectativa del servicio.
* **Impacto Financiero:** DemostraciÃ³n grÃ¡fica de que los contratos "Mes a mes" y los pagos vÃ­a "Cheque electrÃ³nico" concentran el mayor riesgo de fuga de capital de la compaÃ±Ã­a.

> <img width="1428" height="675" alt="Dashboard-PowerBi" src="https://github.com/user-attachments/assets/ac43f8c5-cb2d-48ac-bd15-433ce3e1c305" />
